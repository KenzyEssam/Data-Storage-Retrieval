{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from natsort import natsorted\n",
    "from bs4 import BeautifulSoup  # for HTML parsing\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antoni', 'brutu', 'caeser', 'cleopatra', 'merci', 'worser']\n",
      "['antoni', 'brutu', 'caeser', 'calpurnia']\n",
      "['merci', 'worser']\n",
      "['brutu', 'caeser', 'merci', 'worser']\n",
      "['caeser', 'merci', 'worser']\n",
      "['antoni', 'caeser', 'merci']\n",
      "['angel', 'fool', 'fear', 'in', 'rush', 'to', 'tread', 'where']\n",
      "['angel', 'fool', 'fear', 'in', 'rush', 'to', 'tread', 'where']\n",
      "['angel', 'fool', 'in', 'rush', 'to', 'tread', 'where']\n",
      "['fool', 'fear', 'in', 'rush', 'to', 'tread', 'where']\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(doc):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    stop_words.remove('in')\n",
    "    stop_words.remove('to')\n",
    "    stop_words.remove('where')\n",
    "    \n",
    "    # Remove HTML tags or markup\n",
    "    doc = BeautifulSoup(doc, 'html.parser').get_text()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    doc = doc.lower()\n",
    "\n",
    "    # Remove special characters, keep only alphanumeric characters and spaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc)\n",
    "\n",
    "    tokenized_doc = word_tokenize(doc)\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokenized_doc]\n",
    "\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "\n",
    "    terms = [word for word in stemmed_tokens if word not in stop_words]\n",
    "    return terms\n",
    "\n",
    "files_name = natsorted(os.listdir('files'))\n",
    "document_of_terms = []\n",
    "\n",
    "for files in files_name:\n",
    "    with open(f'files/{files}', 'r') as f:\n",
    "        document = f.read()\n",
    "        document_terms = preprocessing(document)\n",
    "        document_of_terms.append(document_terms)\n",
    "\n",
    "# Print each document on an independent line\n",
    "for document_terms in document_of_terms:\n",
    "    print(document_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional index model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First : Positional index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Index:\n",
      "antoni: [3, {1: [0], 2: [0], 6: [0]}]\n",
      "brutu: [3, {1: [1], 2: [1], 4: [0]}]\n",
      "caeser: [5, {1: [2], 2: [2], 4: [1], 5: [0], 6: [1]}]\n",
      "cleopatra: [1, {1: [3]}]\n",
      "merci: [5, {1: [4], 3: [0], 4: [2], 5: [1], 6: [2]}]\n",
      "worser: [4, {1: [5], 3: [1], 4: [3], 5: [2]}]\n",
      "calpurnia: [1, {2: [3]}]\n",
      "angel: [3, {7: [0], 8: [0], 9: [0]}]\n",
      "fool: [4, {7: [1], 8: [1], 9: [1], 10: [0]}]\n",
      "fear: [3, {7: [2], 8: [2], 10: [1]}]\n",
      "in: [4, {7: [3], 8: [3], 9: [2], 10: [2]}]\n",
      "rush: [4, {7: [4], 8: [4], 9: [3], 10: [3]}]\n",
      "to: [4, {7: [5], 8: [5], 9: [4], 10: [4]}]\n",
      "tread: [4, {7: [6], 8: [6], 9: [5], 10: [5]}]\n",
      "where: [4, {7: [7], 8: [7], 9: [6], 10: [6]}]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Construct a positional index using defaultdict\n",
    "positional_index = defaultdict(lambda: [0, {}])\n",
    "\n",
    "for doc_id, terms in enumerate(document_of_terms, start=1):\n",
    "    term_frequency = {}  # To store the term frequency for each document\n",
    "    for position, term in enumerate(terms):\n",
    "        positional_index[term][1].setdefault(doc_id, []).append(position)\n",
    "        term_frequency[term] = term_frequency.get(term, 0) + 1\n",
    "\n",
    "    # Update document frequency for the current term\n",
    "    for term, freq in term_frequency.items():\n",
    "        positional_index[term][0] += 1\n",
    "\n",
    "# Print the positional index\n",
    "print(\"Positional Index:\")\n",
    "for term, postings in positional_index.items():\n",
    "    print(f\"{term}: {postings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second : Phrase query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document 7', 'document 8', 'document 10']\n"
     ]
    }
   ],
   "source": [
    "def phrase_query(query, positional_index, document_of_terms):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    query_terms = [stemmer.stem(term.lower()) for term in word_tokenize(query) if term.lower() not in stop_words]\n",
    "\n",
    "    if len(query_terms) == 1:  # Single-word query\n",
    "        return [f\"document {doc_id}\" for doc_id in sorted(positional_index.get(query_terms[0], [0, {}])[1].keys())]\n",
    "\n",
    "    result_docs = set(positional_index.get(query_terms[0], [0, {}])[1].keys())\n",
    "\n",
    "    for term in query_terms[1:]:\n",
    "        result_docs &= set(positional_index.get(term, [0, {}])[1].keys())\n",
    "\n",
    "    matching_documents = [\n",
    "        f\"document {doc_id}\"\n",
    "        for doc_id in sorted(result_docs)\n",
    "        if all(pos + 1 == next_pos for pos, next_pos in zip(*[positional_index[term][1].get(doc_id, []) for term in query_terms]))\n",
    "    ]\n",
    "\n",
    "    return matching_documents\n",
    "# Example phrase query\n",
    "example_query = \"fools the fear\"\n",
    "result = phrase_query(example_query, positional_index, document_of_terms)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector space model :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First : Term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           doc1  doc2  doc3  doc4  doc5  doc6  doc7  doc8  doc9  doc10\n",
      "angel         0     0     0     0     0     0     1     1     1      0\n",
      "brutu         1     1     0     1     0     0     0     0     0      0\n",
      "merci         1     0     1     1     1     1     0     0     0      0\n",
      "cleopatra     1     0     0     0     0     0     0     0     0      0\n",
      "fool          0     0     0     0     0     0     1     1     1      1\n",
      "calpurnia     0     1     0     0     0     0     0     0     0      0\n",
      "in            0     0     0     0     0     0     1     1     1      1\n",
      "rush          0     0     0     0     0     0     1     1     1      1\n",
      "caeser        1     1     0     1     1     1     0     0     0      0\n",
      "to            0     0     0     0     0     0     1     1     1      1\n",
      "where         0     0     0     0     0     0     1     1     1      1\n",
      "worser        1     0     1     1     1     0     0     0     0      0\n",
      "tread         0     0     0     0     0     0     1     1     1      1\n",
      "fear          0     0     0     0     0     0     1     1     0      1\n",
      "antoni        1     1     0     0     0     1     0     0     0      0\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'all_terms' is a list of all unique terms across all documents\n",
    "all_terms = list(set(term for document_terms in document_of_terms for term in document_terms))\n",
    "\n",
    "# Create a DataFrame for term frequency (TF)\n",
    "tf_data = {}\n",
    "for i, document_terms in enumerate(document_of_terms, start=1):\n",
    "    term_counts = Counter(document_terms)\n",
    "    tf_data[f'doc{i}'] = [term_counts[term] for term in all_terms]\n",
    "\n",
    "# Create a DataFrame from the TF data\n",
    "tf_df = pd.DataFrame(tf_data, index=all_terms)\n",
    "\n",
    "# Display the transposed TF DataFrame\n",
    "print(tf_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Tf (1+ log tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           doc1  doc2  doc3  doc4  doc5  doc6  doc7  doc8  doc9  doc10\n",
      "angel       0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    0.0\n",
      "brutu       1.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0    0.0\n",
      "merci       1.0   0.0   1.0   1.0   1.0   1.0   0.0   0.0   0.0    0.0\n",
      "cleopatra   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0\n",
      "fool        0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0\n",
      "calpurnia   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0\n",
      "in          0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0\n",
      "rush        0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0\n",
      "caeser      1.0   1.0   0.0   1.0   1.0   1.0   0.0   0.0   0.0    0.0\n",
      "to          0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0\n",
      "where       0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0\n",
      "worser      1.0   0.0   1.0   1.0   1.0   0.0   0.0   0.0   0.0    0.0\n",
      "tread       0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0\n",
      "fear        0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   0.0    1.0\n",
      "antoni      1.0   1.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0    0.0\n"
     ]
    }
   ],
   "source": [
    "def weighted_tf(x):\n",
    "    if x > 0:\n",
    "        return math.log(x) + 1\n",
    "    return 0\n",
    "\n",
    "# Apply the weighted_tf function to the entire DataFrame\n",
    "w_tf_df = tf_df.map(weighted_tf)\n",
    "\n",
    "# Display the transposed weighted TF DataFrame\n",
    "print(w_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second : IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term           \tDF\t     IDF\n",
      "brutu     \t3\t  0.522879\n",
      "merci     \t5\t   0.30103\n",
      "cleopatra \t1\t       1.0\n",
      "caeser    \t5\t   0.30103\n",
      "worser    \t4\t   0.39794\n",
      "antoni    \t3\t  0.522879\n",
      "calpurnia \t1\t       1.0\n",
      "angel     \t3\t  0.522879\n",
      "fool      \t4\t   0.39794\n",
      "in        \t4\t   0.39794\n",
      "rush      \t4\t   0.39794\n",
      "where     \t4\t   0.39794\n",
      "tread     \t4\t   0.39794\n",
      "fear      \t3\t  0.522879\n",
      "to        \t4\t   0.39794\n"
     ]
    }
   ],
   "source": [
    "# Calculate document frequency (DF) and Inverse Document Frequency (IDF) for each term\n",
    "df_data = Counter(term for document_terms in document_of_terms for term in set(document_terms))\n",
    "idf_data = {term: round(math.log10(len(document_of_terms) / df), 6) for term, df in df_data.items()}\n",
    "\n",
    "# Create a DataFrame from the IDF data\n",
    "idf_df = pd.DataFrame(list(idf_data.items()), columns=['Term', 'IDF'])\n",
    "\n",
    "# Print headers\n",
    "print(f\"{'Term':<15}\\t{'DF'}\\t{'IDF':>8}\")\n",
    "\n",
    "# Print the DF and IDF values without the default index\n",
    "for term, df in df_data.items():\n",
    "    idf = idf_data[term]\n",
    "    print(f\"{term:<10}\\t{df:0}\\t{idf:10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third : TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "      <th>doc4</th>\n",
       "      <th>doc5</th>\n",
       "      <th>doc6</th>\n",
       "      <th>doc7</th>\n",
       "      <th>doc8</th>\n",
       "      <th>doc9</th>\n",
       "      <th>doc10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angel</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brutu</th>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merci</th>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleopatra</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fool</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calpurnia</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rush</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caeser</th>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.30103</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worser</th>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.39794</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.39794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tread</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>antoni</th>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               doc1      doc2     doc3      doc4     doc5      doc6      doc7  \\\n",
       "angel      0.000000  0.000000  0.00000  0.000000  0.00000  0.000000  0.522879   \n",
       "brutu      0.522879  0.522879  0.00000  0.522879  0.00000  0.000000  0.000000   \n",
       "merci      0.301030  0.000000  0.30103  0.301030  0.30103  0.301030  0.000000   \n",
       "cleopatra  1.000000  0.000000  0.00000  0.000000  0.00000  0.000000  0.000000   \n",
       "fool       0.000000  0.000000  0.00000  0.000000  0.00000  0.000000  0.397940   \n",
       "calpurnia  0.000000  1.000000  0.00000  0.000000  0.00000  0.000000  0.000000   \n",
       "in         0.000000  0.000000  0.00000  0.000000  0.00000  0.000000  0.397940   \n",
       "rush       0.000000  0.000000  0.00000  0.000000  0.00000  0.000000  0.397940   \n",
       "caeser     0.301030  0.301030  0.00000  0.301030  0.30103  0.301030  0.000000   \n",
       "to         0.000000  0.000000  0.00000  0.000000  0.00000  0.000000  0.397940   \n",
       "where      0.000000  0.000000  0.00000  0.000000  0.00000  0.000000  0.397940   \n",
       "worser     0.397940  0.000000  0.39794  0.397940  0.39794  0.000000  0.000000   \n",
       "tread      0.000000  0.000000  0.00000  0.000000  0.00000  0.000000  0.397940   \n",
       "fear       0.000000  0.000000  0.00000  0.000000  0.00000  0.000000  0.522879   \n",
       "antoni     0.522879  0.522879  0.00000  0.000000  0.00000  0.522879  0.000000   \n",
       "\n",
       "               doc8      doc9     doc10  \n",
       "angel      0.522879  0.522879  0.000000  \n",
       "brutu      0.000000  0.000000  0.000000  \n",
       "merci      0.000000  0.000000  0.000000  \n",
       "cleopatra  0.000000  0.000000  0.000000  \n",
       "fool       0.397940  0.397940  0.397940  \n",
       "calpurnia  0.000000  0.000000  0.000000  \n",
       "in         0.397940  0.397940  0.397940  \n",
       "rush       0.397940  0.397940  0.397940  \n",
       "caeser     0.000000  0.000000  0.000000  \n",
       "to         0.397940  0.397940  0.397940  \n",
       "where      0.397940  0.397940  0.397940  \n",
       "worser     0.000000  0.000000  0.000000  \n",
       "tread      0.397940  0.397940  0.397940  \n",
       "fear       0.522879  0.000000  0.522879  \n",
       "antoni     0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = w_tf_df.multiply(idf_data, axis=0)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "      <th>doc4</th>\n",
       "      <th>doc5</th>\n",
       "      <th>doc6</th>\n",
       "      <th>doc7</th>\n",
       "      <th>doc8</th>\n",
       "      <th>doc9</th>\n",
       "      <th>doc10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>1.373463</td>\n",
       "      <td>1.279619</td>\n",
       "      <td>0.498974</td>\n",
       "      <td>0.782941</td>\n",
       "      <td>0.582747</td>\n",
       "      <td>0.67427</td>\n",
       "      <td>1.223496</td>\n",
       "      <td>1.223496</td>\n",
       "      <td>1.106137</td>\n",
       "      <td>1.106137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            doc1      doc2      doc3      doc4      doc5     doc6      doc7  \\\n",
       "length  1.373463  1.279619  0.498974  0.782941  0.582747  0.67427  1.223496   \n",
       "\n",
       "            doc8      doc9     doc10  \n",
       "length  1.223496  1.106137  1.106137  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_len = pd.DataFrame({'length': np.sqrt((tf_idf**2).sum())}).transpose()\n",
    "\n",
    "doc_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized tf.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "      <th>doc4</th>\n",
       "      <th>doc5</th>\n",
       "      <th>doc6</th>\n",
       "      <th>doc7</th>\n",
       "      <th>doc8</th>\n",
       "      <th>doc9</th>\n",
       "      <th>doc10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angel</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427365</td>\n",
       "      <td>0.427365</td>\n",
       "      <td>0.472707</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brutu</th>\n",
       "      <td>0.380701</td>\n",
       "      <td>0.408621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.667839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merci</th>\n",
       "      <td>0.219176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.603298</td>\n",
       "      <td>0.384486</td>\n",
       "      <td>0.516570</td>\n",
       "      <td>0.446453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleopatra</th>\n",
       "      <td>0.728087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fool</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calpurnia</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.781483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rush</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caeser</th>\n",
       "      <td>0.219176</td>\n",
       "      <td>0.235250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384486</td>\n",
       "      <td>0.516570</td>\n",
       "      <td>0.446453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worser</th>\n",
       "      <td>0.289735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797516</td>\n",
       "      <td>0.508263</td>\n",
       "      <td>0.682869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tread</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427365</td>\n",
       "      <td>0.427365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>antoni</th>\n",
       "      <td>0.380701</td>\n",
       "      <td>0.408621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.775474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               doc1      doc2      doc3      doc4      doc5      doc6  \\\n",
       "angel      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "brutu      0.380701  0.408621  0.000000  0.667839  0.000000  0.000000   \n",
       "merci      0.219176  0.000000  0.603298  0.384486  0.516570  0.446453   \n",
       "cleopatra  0.728087  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "fool       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "calpurnia  0.000000  0.781483  0.000000  0.000000  0.000000  0.000000   \n",
       "in         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "rush       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "caeser     0.219176  0.235250  0.000000  0.384486  0.516570  0.446453   \n",
       "to         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "where      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "worser     0.289735  0.000000  0.797516  0.508263  0.682869  0.000000   \n",
       "tread      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "fear       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "antoni     0.380701  0.408621  0.000000  0.000000  0.000000  0.775474   \n",
       "\n",
       "               doc7      doc8      doc9     doc10  \n",
       "angel      0.427365  0.427365  0.472707  0.000000  \n",
       "brutu      0.000000  0.000000  0.000000  0.000000  \n",
       "merci      0.000000  0.000000  0.000000  0.000000  \n",
       "cleopatra  0.000000  0.000000  0.000000  0.000000  \n",
       "fool       0.325248  0.325248  0.359756  0.359756  \n",
       "calpurnia  0.000000  0.000000  0.000000  0.000000  \n",
       "in         0.325248  0.325248  0.359756  0.359756  \n",
       "rush       0.325248  0.325248  0.359756  0.359756  \n",
       "caeser     0.000000  0.000000  0.000000  0.000000  \n",
       "to         0.325248  0.325248  0.359756  0.359756  \n",
       "where      0.325248  0.325248  0.359756  0.359756  \n",
       "worser     0.000000  0.000000  0.000000  0.000000  \n",
       "tread      0.325248  0.325248  0.359756  0.359756  \n",
       "fear       0.427365  0.427365  0.000000  0.472707  \n",
       "antoni     0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_tf_idf = tf_idf.div(doc_len.iloc[0])\n",
    "\n",
    "norm_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity between query and each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Details\n",
      "        tf  w_tf       idf    tf_idf  normalized\n",
      "antoni   1   1.0  0.522879  0.522879    0.707107\n",
      "brutu    1   1.0  0.522879  0.522879    0.707107\n",
      "\n",
      "Product (query*matched doc)\n",
      "            doc1      doc2\n",
      "antoni  0.269196  0.288939\n",
      "brutu   0.269196  0.288939\n",
      "\n",
      "Product sum\n",
      "doc1    0.538393\n",
      "doc2    0.577877\n",
      "dtype: float64\n",
      "\n",
      "Query Length\n",
      "0.7394625732800816\n",
      "\n",
      "Cosine Similarity\n",
      "doc1    0.538393\n",
      "doc2    0.577877\n",
      "dtype: float64\n",
      "\n",
      "Returned docs\n",
      "doc2 doc1 "
     ]
    }
   ],
   "source": [
    "def get_w_tf(x):\n",
    "    if x > 0:\n",
    "        return math.log10(x) + 1\n",
    "    return 0\n",
    "\n",
    "def insert_query(q):\n",
    "    new_q = preprocessing(q)\n",
    "    \n",
    "    query = pd.DataFrame(index=norm_tf_idf.index)\n",
    "    query['tf'] = [1 if x in new_q else 0 for x in list(norm_tf_idf.index)]\n",
    "    query['w_tf'] = query['tf'].apply(lambda x : get_w_tf(x))\n",
    "    product = norm_tf_idf.multiply(query['w_tf'], axis=0)\n",
    "    query['idf'] = idf_df.set_index('Term').loc[query.index, 'IDF'].values * query['w_tf']\n",
    "    query['tf_idf'] = query['w_tf'] * query['idf']\n",
    "    query['normalized'] = 0\n",
    "\n",
    "    for i in range(len(query)):\n",
    "            query['normalized'] = query['normalized'].astype('float64')\n",
    "            query.loc[new_q, 'normalized'] = query['idf'] / math.sqrt(sum(query['idf'].values**2))\n",
    "    print('Query Details')\n",
    "    print(query.loc[new_q])\n",
    "    \n",
    "    product2 = product.multiply(query['normalized'], axis=0)\n",
    "    scores = {}\n",
    "    column_mapping = {'document ' + str(i): 'doc' + str(i) for i in range(1, 11)}  # Adjust the range as needed\n",
    "\n",
    "    for col in phrase_query(q, positional_index, document_of_terms):\n",
    "       mapped_col = column_mapping.get(col)\n",
    "       if mapped_col:\n",
    "          scores[mapped_col] = product2[mapped_col].sum()\n",
    "\n",
    "    product_result = product2[list(scores.keys())].loc[new_q]\n",
    "\n",
    "    \n",
    "    print('\\nProduct (query*matched doc)')\n",
    "    print(product_result)\n",
    "    \n",
    "    print('\\nProduct sum')\n",
    "    print(product_result.sum())\n",
    "    \n",
    "    print('\\nQuery Length')\n",
    "    q_len = math.sqrt((query['idf'].loc[new_q] ** 2).sum())\n",
    "    print(q_len)\n",
    "    \n",
    "    print('\\nCosine Similarity')\n",
    "    print(product_result.sum())\n",
    "    \n",
    "    print('\\nReturned docs')\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for typle in sorted_scores:\n",
    "        print(typle[0], end=\" \")\n",
    "    \n",
    "    \n",
    "# Example usage\n",
    "query = \"antony brutus\"\n",
    "insert_query(query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
